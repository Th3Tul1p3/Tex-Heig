\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{ulem}

\title{ASD1}
\author{jerome Arn}
\date{19.02.2019}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}
La fiabilité, la robustess, l'extensibilité, la réutilisabilité, la compatibilité et l'efficacité.

\uline{Algorithme:} ensemble fini d'étape dont le but est de résoudre un problème ou d'accomplir une tâche. Ces étapes sont forméesd'un ensemble fini d'opérations. 

\newline Un problème computationnel est spécifié de façon abstraite par :
\bigskip
\begin{itemize}
	\item[I:] Ensemble des entrées 
	\item[O:] Ensemble des sorties
    	\item[R:] Dépendance relationnelle. la sortie en fonction de toutes les entrées possibles
\end{itemize}
\bigskip

\subsection{Lineaire vs. Dichotomique}
Pour 1000 dossiers triés alphabétiquement, une recherche dichotomique demande 1000 consultations dans le pire des cas. Pour le même cas avec une recherche dichotomique, on aura dans le pire des cas 10 consultations. Mais dans certains cas, la recherche est quand même plus rapide. 

\section{Complexité}
Cela définit, en termes de ressources, le besoin d'une algorithme pour être exécutée. L'efficacité se mesur en fonction d'un paramètre n qui caractérise la quantité d'éléments à traiter. La complexité caractérise son coût en temps processeur en fonction de la taille de la données. Cependant on ne cherche pas une mesure précise mais une estimation du nombre d'opérations élémentaires nécessaires. 

\begin{itemize}
	\item[Une incrémentation:]Complexité de O(1)
	\item[une boucle fait N fois:] Complexité de O(N)
    	\item[des boucles imbriquées:] complexité quadratique 
	\item[Boucles avec *n ou /n:] sont de complexité logarithmique
	\item[enchaînement alternatif:] La complexité dans le meilleur des cas, dans le pire des cas et en moyenne
\end{itemize}
\bigskip
Le tri d'un vecteur, par exemple, peut faire appel au probabilités. Dans le meilleur des cas il est déjà trié, dans le pire des cas il est trié à l'envers. Dans ce genre de cas la complexité moyenne est la même que la complexité dans le pire des cas. 
La complexité de la recherche séquentielle est dans le meilleure des cas de O(1) si on trouve directement. Dans le pire des cas on parcout tout l'objet et la complexité est de O(n). Le cas moyen donne $$ (n+1)/2 $$ ce qui vaut à O(n). 

\subsection{Notation de Landau}
Une fonction f est un grand O de la fonction g. Ce qui signifie que f croît au plus vite que g. De cela découle deux règles simple:
\begin{itemize}
	\item[Si la fonction est la somme de plusieurs termes, si l'un deux croît plus vite que les autres, on ne garde que lui et on ignore les autres.]
	\item[Si la fonction est le produit de plusieurs facteurs, on peut ignorer tout facteur constant.]
\end{itemize}
\subsection{Notations complémentaires}
La notation o() est notée quand f(n) croît strictement plus lentement que g(n). La notation \Omega() est utilisée quand f(n) croît au moins aussi vite que g(n). La notation \Theta() est utilisée quand f(n) est du même ordre de grandeur que g(n). 
\section{Structures de données}
Un type de donnée (TDA) abstrait comprend le type de donnée proprement parlé et les opérations permettant de le gérer.
Une pile par exemple offre au moins 4 opérations: push, pop, top et empty. 
Un héritage dans la POO est la dérivation d'une classe de base. Cela donne une nouvelle classe à qui on peut rajouter d'autre propriété sans pour autant modifié celle de la classe de base. 
Les opérations sur une TDA sont classées en 4 catégories:
\begin{itemize}
	\item[Constructeurs:] qui permet de créer des instances.
	\item[Modificateurs:] qui modifient la structure.
	\item[Sélecteurs:] qui interrogent la structure sans la modifier.
	\item[Itérateurs:] qui permettent le parcours de la structure. 
\end{itemize}

\section{Récursivité}
La complexité d'une fonction récursive est en général constanste si on omet les appels récursif. Ce qui veut dire que la complexité final dépend uniquement du nombre d'appel. Dans le cas d'une fonction calculant la factorielle de N, la complexité est de O(N)

Dans le cas de la fonction de Fibonacci, la complexité est de $$ x^(\phi) $$, oû \phi est le nombre d'or. Pour l'algorithme d'Euclide le pire cas est quand une des opérandes est un nombre de Fibonacci et que l'autre est un nombre de Fibonacci de valeur k-1. La complexité dans ce cas est de $$ O(log_(\phi)(n)) $$.Dans le cas des tours de Hanoï, la complexité dépend du nombre et donce du nombre de déplacement. Pour n nombre de disque, on aura N-1 déplacement pour déplacer tous les petits disques du piquet de base au piquet intermédiaire + 1 déplacement pour le grand disque sur le piquet final. 
\end{document}
